{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nsano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nsano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1429</td>\n",
       "      <td>sfa awaits report over mikoliunas the scottish...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sfa await report mikoliuna scottish footbal as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>parmalat to return to stockmarket parmalat  th...</td>\n",
       "      <td>business</td>\n",
       "      <td>parmalat return stockmarket parmalat italian d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1633</td>\n",
       "      <td>edu blasts arsenal arsenal s brazilian midfiel...</td>\n",
       "      <td>sport</td>\n",
       "      <td>edu blast arsen arsen brazilian midfield edu h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2178</td>\n",
       "      <td>henman decides to quit davis cup tim henman ha...</td>\n",
       "      <td>sport</td>\n",
       "      <td>henman decid quit davi cup tim henman retir gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194</td>\n",
       "      <td>french suitor holds lse meeting european stock...</td>\n",
       "      <td>business</td>\n",
       "      <td>french suitor hold lse meet european stock mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1250</td>\n",
       "      <td>blair  damaged  by blunkett row a majority of ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>blair damag blunkett row major voter 68 believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1639</td>\n",
       "      <td>a november to remember last saturday  one news...</td>\n",
       "      <td>sport</td>\n",
       "      <td>novemb rememb last saturday one newspap procla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>916</td>\n",
       "      <td>highbury tunnel players in clear the football ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>highburi tunnel player clear footbal associ sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2217</td>\n",
       "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>top star join us tsunami tv show brad pitt rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>902</td>\n",
       "      <td>eastwood s baby scoops top oscars clint eastwo...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>eastwood babi scoop top oscar clint eastwood m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text  \\\n",
       "0         1429  sfa awaits report over mikoliunas the scottish...   \n",
       "1         1896  parmalat to return to stockmarket parmalat  th...   \n",
       "2         1633  edu blasts arsenal arsenal s brazilian midfiel...   \n",
       "3         2178  henman decides to quit davis cup tim henman ha...   \n",
       "4          194  french suitor holds lse meeting european stock...   \n",
       "..         ...                                                ...   \n",
       "995       1250  blair  damaged  by blunkett row a majority of ...   \n",
       "996       1639  a november to remember last saturday  one news...   \n",
       "997        916  highbury tunnel players in clear the football ...   \n",
       "998       2217  top stars join us tsunami tv show brad pitt  r...   \n",
       "999        902  eastwood s baby scoops top oscars clint eastwo...   \n",
       "\n",
       "          Category                                     Processed_Text  \n",
       "0            sport  sfa await report mikoliuna scottish footbal as...  \n",
       "1         business  parmalat return stockmarket parmalat italian d...  \n",
       "2            sport  edu blast arsen arsen brazilian midfield edu h...  \n",
       "3            sport  henman decid quit davi cup tim henman retir gr...  \n",
       "4         business  french suitor hold lse meet european stock mar...  \n",
       "..             ...                                                ...  \n",
       "995       politics  blair damag blunkett row major voter 68 believ...  \n",
       "996          sport  novemb rememb last saturday one newspap procla...  \n",
       "997          sport  highburi tunnel player clear footbal associ sa...  \n",
       "998  entertainment  top star join us tsunami tv show brad pitt rob...  \n",
       "999  entertainment  eastwood babi scoop top oscar clint eastwood m...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- PRE PROCESSING -----\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stemming tool from nltk\n",
    "stemmer = PorterStemmer()\n",
    "# a mapping dictionary that help remove punctuations\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "def get_tokens(text):\n",
    "  # turn document into lowercase\n",
    "  lowers = text.lower()\n",
    "  # remove punctuations\n",
    "  no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "  # tokenize document\n",
    "  tokens = nltk.word_tokenize(no_punctuation)\n",
    "  # remove stop words\n",
    "  filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "  # stemming process\n",
    "  stemmed = []\n",
    "  for item in filtered:\n",
    "      stemmed.append(stemmer.stem(item))\n",
    "  # final unigrams\n",
    "  return stemmed\n",
    "\n",
    "traningData = pd.read_csv('24_train_3.csv', header='infer')\n",
    "# Use HW 0  token filtering step\n",
    "traningData['Processed_Text'] = traningData['Text'].apply(lambda x: ' '.join(get_tokens(str(x))))\n",
    "traningData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# ----------- TfidfVectorizer method - convert to number - use TFIDF value -----------\n",
    "tfidf_vectorizer = TfidfVectorizer() # like hw 1\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(traningData['Processed_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Bigram Unigram method - cnovert words into matrix of word counts -----------\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))  # shape = 1000, 181043 -- 181043 unique words and bigrams for all text\n",
    "X_count = count_vectorizer.fit_transform(traningData['Processed_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Bert method - convert words into vector -----------\n",
    "# load bert model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "def get_bert_embedding(text):\n",
    "    inputs = bert_tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "        )\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "X_bert_list = []\n",
    "for text in traningData['Processed_Text']:\n",
    "    # get BERT embedding for each text\n",
    "    bert_embedding = get_bert_embedding(text)\n",
    "    X_bert_list.append(bert_embedding)\n",
    "X_bert = np.array(X_bert_list)\n",
    "print(pd.DataFrame(X_bert))\n",
    "\n",
    "#  ----------- GLoVe vector method - convert words into vector -----------\n",
    "glove_vectors = {}\n",
    "# loadin golve vectors\n",
    "with open('glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32') # exclude first elemetn of values\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "def get_glove_embedding(text):\n",
    "    words = get_tokens(text)\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_glove_list = []\n",
    "for text in traningData['Processed_Text']:\n",
    "    # get GloVe embedding for each text\n",
    "    glove_embedding = get_glove_embedding(text)\n",
    "    X_glove_list.append(glove_embedding)\n",
    "X_glove = np.array(X_glove_list)\n",
    "print(pd.DataFrame(X_glove))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate 0.0001\n",
      "Avg Validation Accuracy for lr 0.0001: 0.2340\n",
      "Training with learning rate 0.0003\n",
      "Avg Validation Accuracy for lr 0.0003: 0.3810\n",
      "Training with learning rate 0.001\n",
      "Avg Validation Accuracy for lr 0.001: 0.6390\n",
      "Training with learning rate 0.003\n",
      "Avg Validation Accuracy for lr 0.003: 0.8720\n",
      "Training with learning rate 0.01\n",
      "Avg Validation Accuracy for lr 0.01: 0.9620\n",
      "Training with learning rate 0.03\n",
      "Avg Validation Accuracy for lr 0.03: 0.8820\n",
      "Training with learning rate 0.1\n",
      "Avg Validation Accuracy for lr 0.1: 0.9150\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode labels into numbers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(traningData['Category'])\n",
    "\n",
    "# Convert data to tensors\n",
    "X_tensor = torch.tensor(X_tfidf.toarray(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Define Neural Network Model\n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 5)  # 5 categories\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.output(x)  # No softmax needed, CrossEntropyLoss does it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for lr in [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]:  # Test different learning rates\n",
    "    print(f\"Training with learning rate {lr}\")\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_tensor):\n",
    "        X_train, X_val = X_tensor[train_idx], X_tensor[val_idx]\n",
    "        y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]\n",
    "\n",
    "        model = NewsClassifier(input_size=X_tensor.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)  # Change to SGD or RMSprop to test\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        epochs = 10  # Can increase for better performance\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train)\n",
    "            loss = criterion(output, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate model\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val).argmax(dim=1)\n",
    "            accuracy = (val_preds == y_val).float().mean().item()\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Avg Validation Accuracy for lr {lr}: {sum(accuracies)/len(accuracies):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
